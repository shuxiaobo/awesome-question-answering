# awesome-question-answering

QA

领域经典论文，项目及数据集

#### Papers 
- [Memory Networks](http://arxiv.org/pdf/1410.3916v11.pdf)
- [End-To-End Memory Networks](http://arxiv.org/abs/1503.08895)
- [Towards AI-Complete Question Answering: A set of prerequisite toy tasks](http://arxiv.org/pdf/1502.05698v10.pdf)
- [Large Scale simple question answering with Memory Networks](https://arxiv.org/pdf/1506.02075v1.pdf)
- [Ask Me Anything: Dynamic Memory Networks for Natural Language Processing](http://arxiv.org/pdf/1506.07285v5.pdf)
- [Key-Value Memory Networks for directly understanding documents](https://arxiv.org/pdf/1606.03126v1.pdf)
- [Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf)
- [Value of Semantic Parse Labeling for KBQA](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/acl2016-webqsp.pdf)
- [Question Answering with Subgraph Embeddings](https://arxiv.org/pdf/1406.3676v3.pdf)
- [Open Question Answering with Weakly Supervised Embedding Models](https://arxiv.org/pdf/1404.4326.pdf)
- [Learning End-to-End Goal-Oriented dialog](https://arxiv.org/pdf/1605.07683v2.pdf)
- [End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/IS16_ContextualSLU.pdf)
- [Question Answering over Knowledge Base With Neural Attention Combining Global Knowledge Information](https://arxiv.org/pdf/1606.00979v1.pdf)
- [Compositional Learning of Embeddings for Relation Paths in Knowledge Bases and Texts](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/acl2016relationpaths-1.pdf)
- [Neural Machine Translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473v7.pdf)
- [Recurrent Neural Network Grammar](https://arxiv.org/pdf/1602.07776v4.pdf)
- [Neural Turing Machines](https://www.youtube.com/watch?v=_H0i0IhEO2g)
- [Teaching machines to read and comprehend](https://arxiv.org/pdf/1506.03340.pdf)
- [Applying Deep Learning to answer selection: A study and an open task](https://arxiv.org/pdf/1508.01585v2.pdf)
- [Reasoning with Neural Tensor Networks](https://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.pdf)
- [Scalable Feature Learning for networks: Node2Vec](https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf)
- [Learning Distributed Representations for Rooted Subgraphs from Large Graphs: Subgraph2Vec](https://arxiv.org/pdf/1606.08928.pdf)
- [Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html)
- [Traversing Knowledge Graphs in Vector Space](http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP038.pdf)
- [Learning to Compose Neural Networks for Question Answering](https://arxiv.org/abs/1601.01705)
- [Hierarchical Memory Networks](http://openreview.net/pdf?id=BJ0Ee8cxx)
- [Gaussian Attention Model and its Application to Knowledge Base Embedding and Question Answering](https://arxiv.org/pdf/1611.02266.pdf)
- [Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493)
- [Sequence to Sequence Learning With Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
- [Neural Conversation Model](https://arxiv.org/pdf/1506.05869v1.pdf)
- [Query Reduction Networks For Question Answering](https://arxiv.org/pdf/1606.04582.pdf)
- [Conditional Focused Neural Question Answering with Large-scale Knowledge Bases](https://arxiv.org/pdf/1606.01994.pdf)
- [Efficiently Answering Technical Questions — A Knowledge Graph Approach](http://wangzhongyuan.com/en/papers/Technical_Questions_Answering.pdf)
- [An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge](http://www.nlpr.ia.ac.cn/cip/~liukang/liukangPageFile/ACL2017-Hao.pdf)
- [Question Answering as Global Reasoning over Semantic Abstractions](http://www.cis.upenn.edu/~danielkh/files/2018_semanticilp/2018_aaai_semanticilp.pdf)

#### Category
##### Question generation
- [Question Generation via Overgenerating Transformations and Ranking (Technical report)](https://www.lti.cs.cmu.edu/sites/default/files/cmulti09013.pdf)
- [Automation of question generation from sentences](http://www.sadidhasan.com/sadid-QG.pdf)
- [Good question!statistical ranking for question generation](https://homes.cs.washington.edu/~nasmith/papers/heilman+smith.naacl10.pdf)
- [Question generation from paragraphs at upenn: Qgstec system description](http://www.aclweb.org/anthology/I11-1104)
- [Automatically generating questions from queries for community-based question answering](http://www.aclweb.org/anthology/I11-1104)
- [How to Generate Cloze Questions from Definitions: A Syntactic Approach](https://www.cs.cmu.edu/~listen/pdfs/gates-2011-aaai-qg.pdf)
- [Generating natural language questions to support learning on-line](http://www.aclweb.org/anthology/W13-2114)
- [Deep questions without deep understanding](http://www.aclweb.org/anthology/P15-1086)
- [Leveraging multiple views of text for automatic question generation](http://link.springer.com/chapter/10.1007/978-3-319-19773-9_26)
- [Revup: Automatic gap-fill question generation from educational texts](http://www.aclweb.org/anthology/W15-0618)
- [Towards topic-to-question generation](http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00206)
- [Ranking automatically generated questions using common human queries](http://www.aclweb.org/old_anthology/W/W16/W16-66.pdf#page=233)
- [Generating quiz questions from knowledge graphs](http://delivery.acm.org/10.1145/2750000/2742722/p113-seyler.pdf)
- [Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus](http://arxiv.org/pdf/1603.06807v1.pdf)
- [Knowledge Questions from Knowledge Graphs](https://arxiv.org/abs/1610.09935)
- [Machine Comprehension by Text-to-Text Neural Question Generation](http://aclweb.org/anthology/W17-2603)
- [Question Generation from a Knowledge Base with Web Exploration](https://arxiv.org/pdf/1610.03807.pdf)
- [On Generating Characteristic-rich Question Sets for QA Evaluation](http://www.aclweb.org/anthology/D/D16/D16-1054.pdf)
- [Neural Question Generation from Text: A Preliminary Study](https://arxiv.org/pdf/1704.01792.pdf)
- [Semi-supervised qa with generative domain-adaptive nets](https://pdfs.semanticscholar.org/e8a0/536dc080acd2ca83502dddd0d511ef3fbd8c.pdf)


#### Datasets
> 完型填空

- [CliCR (2018) **On going**](https://github.com/clips/clicr)
一个医学数据集，完型填空式的
**数据量：**
**数据特点：**
**评价指标：**
**SOTA: 	EM:39.77 F1:69.12**

- [CODAH (2019) **On going**](https://github.com/Websail-NU/CODAH) 
对抗生成的数据集，根据18年前SOTA的模型预测错误的问答对生成的，问答对主要包含的是一些常识推理的数据，给定一个句子的一部分，推测不完整部分内容，选择题，也是完型填空内容。
**数据量：**
**数据特点：**
**评价指标：**

> 抽取式问答
 
 - [HotpotQA (2018) **On going**](https://hotpotqa.github.io/)
 主要针对基于多个信息内容的多步推理 (multi-hop reasoning)
    1. 问题的答案必须要基于多个支持文档；
    2. 问题多样化，不局限于任何已有的知识库或知识模式；
    3. 提供句子级别的支持推理线索（supporting fact），允许 QA 系统用强大的监督进行推理，并对预测结果进行解释；
    4. 提供了新型模拟比较型问题，来测试 QA 系统提取相关线索、执行必要对比的能力。
**SOTA: 	EM:39.77 F1:69.12**
**数据量：112779条数据，问人名机构名和地点的问题多，大多数推理依赖于实体的推理**
**数据特点：多跳推理，事实标注，不依赖于KB。[其中test-distractor挑战模型存在噪声时找到真实支持事实，test-fullwiki通过要求它回答所有维基百科文章的第一段没有指定黄金段落的问题来充分测试模型找到相关事实以及推理相关事实的能力](https://zhuanlan.zhihu.com/p/61909318)**
**评价指标：**

- [NEWSQA: A MACHINE COMPREHENSION DATASET (2018)](https://datasets.maluuba.com/NewsQA)
NewQA 提供了超过 10 万经过人工标注得到的问题-答案 (question-answer)对。 这些问题和答案来自于美国 CNN 的 10,000 多篇新闻文章，答案还包括了相应文章以及文字段落。数据集的收集包含了 4 个阶段，旨在得到那些经过推理 (reasoning) 才能回答的问题。值得一提的是，相比较于由 Stanford NLP Group 整理的 SQuAD，NewsQA 包含了更多的文章和问题，而且需要复杂推理的问题也比 SQuAD 更多 (33.9% v.s 20.5%)。且文本长度更长。
**数据量：100,000条数据来源于CNN，都是人工生成数据，问题和答案不是同一个人生成的**
**数据特点：**
**评价指标：**

- [QuAC (2018)][http://quac.ai/quac_poster_pdf.pdf]
数据集生成自对话数据中，所以问题是有上下文的，答案的包括从文本中选取，或者无法回答
**数据量：100,000条数据来源于CNN，都是人工生成数据，问题和答案不是同一个人生成的**
**数据特点：并不是纯span，80%是要基于上下文的，其中40%是要基于对话历史，60%要基于文档。20%没有答案。40%的问题答案来自多个span。**
**评价指标：F1, HEQQ,	HEQD**
**SOTA：69.4	65.4	9.3**


- [TriviaQA]
在 TriviaQA 中，每个问题通常附带了多个相关文档用来获取答案（这些文档是通过信息检索的方式得到的）。然而相关文档的增多并不能保证回答某个问题一定用到跨文档的多步推理，实际上，该数据库中大部分问题仍旧可以只通过多个文档中的某一个直接回答。
**数据量：**
**数据特点：**
**评价指标：**

- [QAngaroo (2018)](http://qangaroo.cs.ucl.ac.uk/leaderboard.html)
 QAngaroo 利用知识图谱技术构建了一批确实需要多步推理才能回答的问题。然而，该数据集的问题和答案的种类严重受限于知识图谱预先定义的模式 (schema) 本身，同时问题格式也被限制为知识图谱的三元组形式（triple）而非自然语言。此外，以上提到的所有数据集在给出相关问题的同时仅提供相关文档本身，并没有给出更细粒度和更直接的推理线索。数据的来源是wiki和一个化学数据
**数据量：**
**数据特点：需要实体之间关系的多轮推理**
**评价指标：ACC**
**SOTA：75%**

- [DuoRC (2018) From IBM](https://duorc.github.io/#examples)
这个数据集的主要目的是想引入外部知识和推理来完成回答问题。
**数据量：186,089条问答对**
**数据特点：需要实体之间关系的多轮推理，问题和答案单词尽量少的重合**
**评价指标：ACC, F1**
**SOTA：**

- [QUASAR (2017)]()
数据来自stackoverflow，数据分为两部分：37,000个阅读理解填空式的。43,000个抽取式问答。
**数据量：37,000个阅读理解填空式的。43,000个抽取式问答**
**数据特点：需要实体之间关系的多轮推理，问题和答案单词尽量少的重合**
**评价指标：EM, F1**
**SOTA：75%**


> 对话式/生成式问答 

- [CoQA (2018) **Closed**](https://www.jiqizhixin.com/articles/2018-09-11-3)  
介绍了一个用于构建对话问答系统的新数据集 CoQA。该数据集包含来自 7 个不同领域的文本段落里 8000 个对话中的 127,000 轮问答。问题是会话形式，而答案是自由形式的文本，并在段落中突出显示相应的依据。我们深入分析了 CoQA，发现会话问题具有现有阅读理解数据集所没有的挑战性现象，例如共指关系和实用推理。三个目标：1. 问题之间上下文的关联。2. 在 CoQA 中，答案可以是自由形式的文本（抽象答案），而提取跨度则作为实际答案的参照。3. 构建跨域稳定执行的 QA 系统，研究人员从七个不同的领域收集数据集——儿童故事、文学、中学和高中英语考试、新闻、维基百科、科学和 Reddit。最后两个用于域外评估。
总而言之，CoQA 具有以下主要特征：
通过文本段落从 8000 个对话中收集了 127，000 轮问答（每段约一个对话）。平均会话长度为 15 回合，每回合包含一个问题和一个答案。
自由形式的答案。每个答案都有一个提取理由，在段落中突出显示。
文本段落来自七个不同的领域——五个用于域内评估，两个用于域外评估。
**数据量：1,010,916 Real Bing User Queries
    182,669 Natural Language Answers.
    No Answer Subset
    10 Passages Per Query**
**数据特点：**
**评价指标： ROUGE-L and BLEU-1.**

- [MS MARCO (2016)]()
建立在经过匿名处理的真实世界数据基础之上。一种生成式问答，数据来源于bing和小娜的真实搜索。数据量大。
**数据量：1,010,916 Real Bing User Queries
182,669 Natural Language Answers.
No Answer Subset
10 Passages Per Query**
**数据特点：**
**评价指标： ROUGE-L and BLEU-1.**
**SOTA:	 ROUGE: 0.540	Bleu: 0.565**

- [DuReader (2018)]
一个中文的问答数据集，数据来自百度知道和百科，开放域的问答
**数据量：300K 问题, 660K 答案 1.5M 文档;**
**数据特点：需要实体之间关系的多轮推理，问题和答案单词尽量少的重合**
**评价指标：ROUGE-L	BLEU-4**
**SOTA：63.48	61.54**

- [NarrativeQA (2017) Deepmind]
目前存在的RC 数据集和任务中的question能够通过superficial information得到答案，如local context similarity, global term frequency.所以无法评估model的理解能力。

数据量够大，用于支持word embedding训练和满足lexical coverage and diversity
问题和答案应该是自然语言描述的，答案应该是从多个片段或一大段话中总结出来的 标注者要用自己的语言总结答案，要从更高层次考虑实体间的关系，而非直接从原文中截取答案
能从答案流畅性和正确性两个角度评估model
数据集的规模和复杂性应该控制住现有model难以很好解决，但是人类可以解决的范围内

**数据量：300K 问题, 660K 答案 1.5M 文档;**
**数据特点：需要实体之间关系的多轮推理，问题和答案单词尽量少的重合**
**评价指标：Bleu-1，Bleu-4，Rouge-L, MRR**
**SOTA：63.48	61.54**

> Closed

- [bAbI dataset **Closed**](https://research.facebook.com/research/babi/)
- [CNN QA Task (Teaching Machines to Read & Comprehend)(2015 **Closed**)](https://github.com/deepmind/rc-data/)
- [WebQuestions(2013 **Closed**)](http://nlp.stanford.edu/software/sempre/)
- [Simple Questions(2015) **Closed**](https://research.facebook.com/research/babi)
- [Movie QA(2016) **Closed**](https://research.facebook.com/research/babi/)
- [WebQuestionsSP (2016) **Closed**](https://www.microsoft.com/en-us/download/details.aspx?id=52763)
- [WikiQA (2016) **Closed**](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/YangYihMeek_EMNLP-15_WikiQA.pdf)
- [Kaggle AllenAI Challenge **Closed**](https://www.kaggle.com/c/the-allen-ai-science-challenge)
- [MC Test, Machine Comprehension Test Microsoft (2013) **Closed**](http://research.microsoft.com/en-us/um/redmond/projects/mctest/)
- [MSR Sentence Completion Challenge (2013) **Closed**](https://www.microsoft.com/en-us/research/project/msr-sentence-completion-challenge/)
- [Dialog State Tracking Challenge (2014) **Closed**](http://camdial.org/~mh521/dstc/)
- [QA dataset featured in Teaching Machines to Read and Comprehend (2015) **Closed**](https://github.com/deepmind/rc-data/)
- [WebNav (2016)](https://github.com/nyu-dl/WebNav/blob/master/README.md)
- [Cornell Movie Dialogue Dataset (2011) **Closed**](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)
- [WikiQA (2015) **Closed**](http://aka.ms/WikiQA)
- [Quora Duplicate Questions Dataset **Closed**](https://data.quora.com/)
- [Quiz Bowl Questions (2015) **Closed**](https://www.cs.colorado.edu/~jbg/projects/IIS-1320538.html#Datasets)
- [WebQA-Chinese (2015) **Closed**](http://idl.baidu.com/WebQA.html)
- [Chat corpus (2015) **Closed**](https://github.com/Marsan-Ma/chat_corpus)
- [MultiRC](http://cogcomp.org/multirc/)
- [FreebaseQA]()
- [Stanford Question Answering Dataset 2.0 (2018)](https://rajpurkar.github.io/SQuAD-explorer/)
- [Query Reformulator Dataset Jeopardy etc (2017) ](https://github.com/nyu-dl/QueryReformulator)

### Compare to other MRC datasets

| **Dataset**     | **Segment** | **Query Source** | **Answer**         | **Queries** | **Document**              |
| :-------------- | :---------- | :-------------- | :----------------- | :---------- | :----------------------- |
| MCTest          | No          | Crowd-sourced    | Multi-choices      | 2.6K        | 660                       |
| RACE            | No          | Crowd-sourced    | Multi-choices      | 97K         | 28K                       |
| ARC         | No          | Generated        | Multi-choices      | 8K          | 14M sentences             |
| WikiQA          | No          | User logs        | Sentence selection | 3K          | 29.26K sentences          |
| CNN/Daily Mail  | No          | Close            | Fill in entity     | 1.4M        | 93K CNN, 220K DM          |
| ReCoRD      | No          | Close            | Fill in entity     | 12K         | 12K                       |
| Children’s Book | No          | Close            | Fill in the word   | 688K        | 688K contexts, 108 books  |
| SQuAD           | No          | Crowd-sourced    | Span               | 100K        | 536                       |
| SQuAD2.0 | No | Crowd-sourced | No answer/Span | 150K | 536 |
| NewsQA          | No          | Crowd-sourced    | Span               | 100K        | 10K                       |
| SearchQA        | No          | Generated        | Span               | 140K        | 6.9M passages             |
| ***HotpotQA*** | No | Crowd-sourced | Yes/No/Span | 113K | 5M wiki paragraphs |
| DuReader        | No          | Crowd-sourced    | Human generated    | 200K        | 1M                        |
| NarrativeQA     | No          | Crowd-sourced    | Human generated    | 47K         | 1572 stories              |
| ***CoQA*** | No | Crowd-sourced | Human generated | 127K | 8000 conversations |
| ***QuAC*** | No | Crowd-sourced | Human generated | 100K | Daffy Duck’s Wiki page |
| MS MARCO V1     | **Yes**     | User logs        | Human generated    | 100K    | 1M passages, 200K docs.   |
| **MS MARCO V2** | **Yes**     | User logs        | Human generated    | **1M**      | 8.8M passages, 3.2M docs. |


#### KBs
- [NetBase](https://github.com/pannous/netbase)
- [Freebase](https://developers.google.com/freebase/)

#### Presentations
- [Relation Learning for Large Scale Knowledge Graph](http://nlp.csai.tsinghua.edu.cn/~lzy/talks/adl2015.pdf)
- [Attention and Memory](http://videolectures.net/site/normal_dl/tag=1051694/deeplearning2016_chopra_attention_memory_01.pdf)

#### LM Datasets
- PennTree Bank
- Text8

#### Code & Relevant Projects
- [MemNN Impl Matlab](https://github.com/facebook/MemNN)
- [Key Value MemNN](https://github.com/siyuanzhao/key-value-memory-networks)
- [Quepy](https://github.com/machinalis/quepy)
- [NLQuery](https://github.com/ayoungprogrammer/nlquery)
- [ParlAI](https://github.com/facebookresearch/ParlAI)
- [flask-chatterbot](https://github.com/chamkank/flask-chatterbot)
- [Learning to Rank short text pairs with CNN SIGIR 2015](https://github.com/shashankg7/Keras-CNN-QA)
- [TextKBQA](https://github.com/rajarshd/TextKBQA)
- [BiAttnFlow](https://github.com/allenai/bi-att-flow)


